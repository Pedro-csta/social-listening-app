import streamlit as st
import pandas as pd
import json
import io
from docx import Document
from youtube_comment_downloader import YoutubeCommentDownloader
import re
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from wordcloud import WordCloud
import os
import google.generativeai as genai
import numpy as np

# --- CONFIGURA√á√ÉO GEMINI ---
gemini_api_key = st.secrets.get("GOOGLE_API_KEY")
if not gemini_api_key:
    st.error("A chave da API do Google Gemini n√£o foi encontrada. Configure-a no arquivo `secrets.toml` do seu projeto Streamlit.")
    st.stop()

genai.configure(api_key=gemini_api_key)
model = genai.GenerativeModel('gemini-1.5-flash')

# --- CONSTANTE PARA LIMITE DE COMENT√ÅRIOS ---
MAX_COMMENTS_TO_PROCESS = 1000

# --- FUN√á√ïES DE EXTRA√á√ÉO DE DADOS ---
@st.cache_data(show_spinner="Extraindo texto do arquivo...")
def extract_text_from_file(file_contents, file_extension):
    text_content_list = []
    try:
        if file_extension == '.csv':
            df = pd.read_csv(io.StringIO(file_contents.decode('utf-8')))
            if 'comentario' in df.columns: text_content_list = df['comentario'].dropna().astype(str).tolist()
            else: text_content_list = [" ".join(row.dropna().astype(str)) for _, row in df.iterrows()]
        elif file_extension in ['.xls', '.xlsx']:
            df = pd.read_excel(io.BytesIO(file_contents))
            if 'comentario' in df.columns: text_content_list = df['comentario'].dropna().astype(str).tolist()
            else: text_content_list = [" ".join(row.dropna().astype(str)) for _, row in df.iterrows()]
        elif file_extension in ['.doc', '.docx']:
            document = Document(io.BytesIO(file_contents))
            text_content_list = [p.text for p in document.paragraphs if p.text.strip()]
    except Exception as e: st.error(f"Erro ao extrair texto do arquivo: {e}")
    return text_content_list

@st.cache_data(show_spinner=False)
def download_youtube_comments(youtube_url):
    all_comments = []
    try:
        downloader = YoutubeCommentDownloader()
        match = re.search(r'(?:v=|/)([0-9A-Za-z_-]{11}).*', youtube_url)
        if not match: st.error(f"URL do YouTube inv√°lida: {youtube_url}."); return []
        video_id = match.group(1)
        with st.spinner(f"Baixando coment√°rios de: {youtube_url}..."):
            comments_generator = downloader.get_comments(video_id)
            for comment in comments_generator:
                if comment and 'text' in comment: all_comments.append(comment['text'])
                if len(all_comments) >= 2500: break
    except Exception as e: st.error(f"N√£o foi poss√≠vel baixar coment√°rios de '{youtube_url}': {e}")
    return all_comments

# --- FUN√á√ïES DE AN√ÅLISE COM GEMINI ---
def clean_json_response(text):
    match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', text, re.DOTALL)
    if match: return match.group(1)
    match = re.search(r'^\s*\{[\s\S]*?\}\s*$', text, re.DOTALL)
    if match: return match.group(0)
    return text

@st.cache_data(show_spinner="Analisando texto com a IA...")
def analyze_text_with_gemini(_text_to_analyze):
    if not _text_to_analyze.strip(): return None
    prompt = f"""Analise os coment√°rios e retorne um √∫nico objeto JSON com a estrutura: {{"sentiment": {{"positive": float, "neutral": float, "negative": float, "no_sentiment_detected": float}}, "topics": [{{"name": "Nome", "positive": int, "neutral": int, "negative": int}}], "term_clusters": {{"termo1": int}}, "topic_relations": [{{"source": "Tema A", "target": "Tema B", "description": "Desc."}}]}}. Instru√ß√µes: 1. `sentiment`: porcentagens, soma 100. 2. `topics`: 5-10 temas principais com contagem de sentimentos. 3. `term_clusters`: 10-20 termos significativos e frequ√™ncia. 4. `topic_relations`: 3-5 pares de temas relacionados. Texto: "{_text_to_analyze}" """
    try:
        response = model.generate_content(prompt)
        data = json.loads(clean_json_response(response.text))
        return data
    except Exception as e: st.error(f"Erro na an√°lise principal da IA: {e}"); return None

# --- FUN√á√ïES DE AN√ÅLISE CONTEXTUAL ---
@st.cache_data
def generate_sentiment_analysis_text(_sentiment_data):
    prompt = f"""Aja como um analista de dados s√™nior. Com base na seguinte distribui√ß√£o de sentimentos: {json.dumps(_sentiment_data)}. Escreva uma an√°lise profissional de 1 a 2 par√°grafos sobre o que essa distribui√ß√£o sugere sobre a recep√ß√£o do p√∫blico, as implica√ß√µes de neg√≥cio e o tom geral da conversa."""
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception: return "N√£o foi poss√≠vel gerar a an√°lise textual para o gr√°fico de sentimento."

@st.cache_data
def generate_topics_analysis_text(_topics_data):
    prompt = f"""Aja como um Analista de Mercado e Estrategista de Neg√≥cios S√™nior. Sua tarefa √© transformar os dados brutos de temas e sentimentos em um memorando estrat√©gico coeso e acion√°vel para as equipes de marketing e produto. Dados: {json.dumps(_topics_data, indent=2, ensure_ascii=False)}. Escreva um texto corrido e natural, sem quebras por t√≥picos ou bullet points. A an√°lise deve fluir como um relat√≥rio coeso, abordando: 1. S√≠ntese Estrat√©gica: A hist√≥ria central que os temas contam. 2. An√°lise dos Pontos Fortes: Temas positivos e como o marketing pode alavanc√°-los. 3. An√°lise dos Desafios e Oportunidades: Temas negativos/neutros e o que revelam para o produto. 4. Recomenda√ß√£o Final: Onde focar a aten√ß√£o estrat√©gica no curto prazo. O objetivo √© que este texto seja 'pronto para enviar' para um gestor de produto ou marketing."""
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception: return "N√£o foi poss√≠vel gerar a an√°lise textual para o gr√°fico de temas."

@st.cache_data
def generate_wordcloud_analysis_text(_term_clusters_data):
    prompt = f"""Aja como um pesquisador de mercado. Os seguintes termos foram os mais frequentes nos coment√°rios: {json.dumps(list(_term_clusters_data.keys()))}. Escreva uma an√°lise profissional de 1 a 2 par√°grafos sobre a hist√≥ria que esses termos contam, o que √© "top of mind" para o p√∫blico e o que o vocabul√°rio revela sobre seu perfil."""
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception: return "N√£o foi poss√≠vel gerar a an√°lise textual para a nuvem de palavras."

@st.cache_data
def generate_relations_analysis_text(_relations_data):
    prompt = f"""Aja como um analista de sistemas de neg√≥cio. As seguintes rela√ß√µes entre temas foram identificadas: {json.dumps(_relations_data)}. Escreva uma an√°lise profissional de 1 a 2 par√°grafos sobre o que as conex√µes entre os temas revelam sobre a jornada do usu√°rio e quais as implica√ß√µes estrat√©gicas disso."""
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception: return "N√£o foi poss√≠vel gerar a an√°lise textual para o grafo de rela√ß√µes."

@st.cache_data
def generate_qualitative_analysis(_analysis_results, _text_sample):
    prompt = f"""Como especialista em Marketing, redija uma an√°lise qualitativa (3-4 par√°grafos) com base nos dados. Foco em insights, temas e oportunidades. Dados: {json.dumps(_analysis_results, ensure_ascii=False)}"""
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception as e: return f"Erro: {e}"

# --- PROMPT ATUALIZADO COM EXEMPLO PARA A PERSONA SINT√âTICA ---
@st.cache_data
def generate_persona_insights(_analysis_results, _text_sample):
    prompt = f"""
    Aja como um Estrategista de Marketing e Produto de classe mundial, especialista em destilar dados brutos em perfis de persona acion√°veis.

    Sua tarefa √© criar um perfil de persona sint√©tica extremamente detalhado e estrat√©gico, com base nos dados de social listening fornecidos.

    **Dados da An√°lise:**
    {json.dumps(_analysis_results, ensure_ascii=False)}

    ---

    **EXEMPLO DE ESTRUTURA E QUALIDADE PERFEITA:**

    **Persona Identificada:** O Investidor Global Consciente

    **1. Perfil Resumido:**
    O Investidor Global Consciente √© um profissional ou pessoa com renda dispon√≠vel que busca diversificar seus investimentos internacionalmente, priorizando a otimiza√ß√£o de custos e a transpar√™ncia. Ele √© pragm√°tico e busca informa√ß√µes detalhadas antes de tomar decis√µes, demonstrando preocupa√ß√£o com impostos e regulamenta√ß√µes.

    **2. Dores e Necessidades Principais:**
    * Taxas e Custos elevados: A principal preocupa√ß√£o √© com taxas de corretagem, IOF, e spread, buscando solu√ß√µes mais econ√¥micas.
    * Complexidade da Declara√ß√£o de Imposto de Renda sobre investimentos no exterior: Falta clareza e facilidade no processo de declara√ß√£o de ganhos de capital e dividendos obtidos no exterior.
    * Falta de transpar√™ncia e suporte adequado de algumas corretoras: Experi√™ncias negativas com o suporte ao cliente e dificuldades em resolver problemas com corretoras geram desconfian√ßa.

    **3. Desejos e Motiva√ß√µes:**
    * Investimentos no exterior com baixo custo: Busca plataformas e estrat√©gias para investir internacionalmente com as menores taxas e custos poss√≠veis.
    * Acesso a informa√ß√µes claras e precisas: Necessita de informa√ß√µes detalhadas e transparentes sobre investimentos, impostos e regulamenta√ß√µes.
    * Diversifica√ß√£o de portf√≥lio com ETFs: Interesse em ETFs como IVV e IVVB11 para diversifica√ß√£o internacional.

    **4. Tom de Voz e Comportamento:**
    O Investidor Global Consciente se comunica de forma pragm√°tica e direta, valorizando informa√ß√µes objetivas e dados quantitativos. Ele √© c√©tico em rela√ß√£o a promessas exageradas e busca comprova√ß√£o de resultados. Realiza pesquisas detalhadas antes de tomar decis√µes, comparando diversas op√ß√µes e lendo avalia√ß√µes de outros usu√°rios.

    **5. Oportunidades de Engajamento:**
    * Desenvolver um guia completo e atualizado sobre a declara√ß√£o de imposto de renda para investimentos no exterior, com checklists e exemplos pr√°ticos.
    * Lan√ßar uma calculadora online que simule os custos de investimento em diferentes plataformas, incluindo taxas, IOF e spread.
    * Criar webinars e workshops gratuitos com especialistas em investimentos internacionais e planejamento tribut√°rio.

    ---

    **Sua Tarefa Agora:**
    Com base nos dados da an√°lise fornecidos no in√≠cio deste prompt, gere um novo perfil de persona, seguindo **EXATAMENTE** a mesma estrutura, formato e n√≠vel de profundidade do exemplo acima.
    """
    try:
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        return f"Erro ao gerar insights de persona: {e}"


@st.cache_data
def generate_ice_score_tests(_analysis_results):
    prompt = f"""Como Growth Hacker, sugira 10 testes (ICE Score), ordenados. Resposta DEVE ser um √∫nico JSON. Inclua: Ordem, Nome, Descri√ß√£o, Vari√°vel, Impacto (1-10), Confian√ßa (1-10), Facilidade (1-10), ICE Score. Dados: {json.dumps(_analysis_results, ensure_ascii=False)}"""
    try: response = model.generate_content(prompt); return json.loads(clean_json_response(response.text))
    except Exception as e: st.error(f"Erro ao gerar testes ICE: {e}"); return None

@st.cache_data
def generate_product_marketing_insights(_analysis_results):
    prompt = f"""
    Aja como um Consultor de Estrat√©gia de Produto e Marketing de alto n√≠vel. Sua tarefa √© destilar os dados de social listening em um briefing executivo, conciso e extremamente acion√°vel. A filosofia √© 'menos √© mais', mas com profundidade. Evite repeti√ß√µes de outras se√ß√µes e foque nas conclus√µes estrat√©gicas.

    **Dados da An√°lise:**
    {json.dumps(_analysis_results, ensure_ascii=False)}

    **Estruture sua resposta em Markdown, usando exatamente estes tr√™s t√≥picos:**

    ### Briefing Estrat√©gico para Produto e Marketing

    **1. Diagn√≥stico Central:**
    * **An√°lise Profunda (2 par√°grafos):** Considerando a an√°lise de um grande volume de coment√°rios, v√° al√©m da superf√≠cie. Sintetize os dados para revelar a hist√≥ria central.
        * **Qual √© a principal for√ßa positiva?** Descreva o principal desejo ou motiva√ß√£o que atrai o p√∫blico a este t√≥pico/produto.
        * **Qual √© a principal barreira ou fonte de atrito?** Descreva a maior dor, medo ou frustra√ß√£o que o p√∫blico enfrenta.
        * **Qual a tens√£o estrat√©gica resultante?** Explique o conflito central que surge do choque entre o desejo e a barreira. Esta tens√£o √© a oportunidade de neg√≥cio mais importante a ser resolvida.

    **2. Diretrizes Estrat√©gicas para MARKETING:**
    * Com base no diagn√≥stico, liste de 2 a 3 recomenda√ß√µes de alto impacto para a equipe de marketing. Seja prescritivo.

    **3. Diretrizes Estrat√©gicas para PRODUTO:**
    * Com base no diagn√≥stico, liste de 2 a 3 recomenda√ß√µes de alto impacto para a equipe de produto. Seja prescritivo.

    O objetivo √© criar um documento que um C-level ou l√≠der de equipe possa ler em 60 segundos e entender exatamente onde focar os esfor√ßos.
    """
    try: response = model.generate_content(prompt); return response.text.strip()
    except Exception as e: return f"Erro ao gerar insights de PMM: {e}"

# --- FUN√á√ïES DE VISUALIZA√á√ÉO ---
def plot_sentiment_chart(sentiment_data):
    colors_for_pie = {'positive': '#4CAF50', 'neutral': '#cccccc', 'negative': '#ffcd03', 'no_sentiment_detected': '#f0f0f0'}
    labels_order = ['positive', 'neutral', 'negative', 'no_sentiment_detected']
    display_labels = ['Positivo', 'Neutro', 'Negativo', 'N√£o Detectado']
    sizes = [sentiment_data.get(label, 0.0) for label in labels_order]
    filtered_data = [(display_labels[i], sizes[i], colors_for_pie[labels_order[i]]) for i, size in enumerate(sizes) if size > 0]
    if not filtered_data: st.warning("Dados de sentimento insuficientes."); return
    filtered_labels, filtered_sizes, filtered_colors = zip(*filtered_data)
    fig, ax = plt.subplots(figsize=(6, 6))
    wedges, texts, autotexts = ax.pie(filtered_sizes, explode=[0.03]*len(filtered_labels), labels=filtered_labels, colors=filtered_colors, autopct='%1.1f%%', startangle=90, pctdistance=0.85)
    for autotext in autotexts: autotext.set_color('black'); autotext.set_fontweight('bold')
    ax.add_artist(plt.Circle((0,0),0.70,fc='white'))
    ax.axis('equal'); ax.set_title('1. An√°lise de Sentimento Geral', pad=18, color='#1f2329')
    st.pyplot(fig)

def plot_topics_chart(topics_data):
    if not topics_data: st.warning("Dados de temas insuficientes."); return
    df_topics = pd.DataFrame(topics_data).fillna(0)
    df_topics['Total'] = df_topics['positive'] + df_topics['neutral'] + df_topics['negative']
    df_topics = df_topics.sort_values('Total', ascending=True)
    fig, ax = plt.subplots(figsize=(8, max(4, len(df_topics) * 0.5)))
    df_topics[['positive', 'neutral', 'negative']].plot(kind='barh', stacked=True, color=['#4CAF50', '#ffcd03', '#000000'], ax=ax)
    ax.set_yticklabels(df_topics['name'])
    ax.set_title('2. Temas Mais Citados por Sentimento'); ax.set_xlabel('N√∫mero de Coment√°rios'); ax.set_ylabel('Tema')
    ax.legend(['Positivo', 'Neutro', 'Negativo'], loc='lower right', frameon=False)
    plt.tight_layout(); st.pyplot(fig)

def plot_word_cloud(term_clusters_data):
    if not term_clusters_data: st.warning("Dados de termos insuficientes."); return
    color_func = lambda *args, **kwargs: "#ffcd03" if np.random.rand() > 0.7 else "#1f2329"
    wordcloud = WordCloud(width=700, height=400, background_color='white', color_func=color_func, collocations=False).generate_from_frequencies(term_clusters_data)
    fig = plt.figure(figsize=(8, 5)); plt.imshow(wordcloud, interpolation='bilinear'); plt.axis('off'); plt.title('3. Agrupamento de Termos')
    st.pyplot(fig)

def plot_topic_relations_chart(topic_relations_data):
    if not topic_relations_data: st.warning("Dados de rela√ß√µes insuficientes."); return
    G = nx.Graph()
    for rel in topic_relations_data:
        if rel.get('source') and rel.get('target'): G.add_edge(rel['source'], rel['target'])
    if not G.edges(): st.warning("Nenhuma rela√ß√£o v√°lida encontrada."); return
    fig, ax = plt.subplots(figsize=(8, 7)); pos = nx.spring_layout(G, k=0.7, iterations=50, seed=42)
    nx.draw_networkx_nodes(G, pos, node_size=2000, node_color='#ffcd03', alpha=0.9, ax=ax)
    nx.draw_networkx_edges(G, pos, width=1.2, edge_color='#1f2329', alpha=0.6, ax=ax)
    nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', font_color='#000000', ax=ax)
    ax.set_title('4. Rela√ß√£o Entre Temas'); plt.axis('off'); plt.tight_layout(); st.pyplot(fig)

# --- LAYOUT DA APLICA√á√ÉO STREAMLIT ---
st.set_page_config(layout="wide", page_title="Social Listening Tool + AI")
st.title("üó£Ô∏è Social Listening Tool + AI")
st.markdown("---")

st.markdown(f"Carregue uma base de coment√°rios, insira at√© 3 URLs de v√≠deos do YouTube ou cole coment√°rios abaixo. A an√°lise da IA ser√° feita com uma amostra de at√© **{MAX_COMMENTS_TO_PROCESS} coment√°rios**.")

if 'last_input_type' not in st.session_state: st.session_state.last_input_type = None
col1, col2 = st.columns(2)
with col1:
    uploaded_file = st.file_uploader("Upload de arquivo:", type=["csv", "xls", "xlsx", "doc", "docx"], key="fileuploader")
    if uploaded_file: st.session_state.last_input_type = 'file'
with col2:
    st.write("**Ou insira at√© 3 URLs do YouTube:**")
    youtube_url_1 = st.text_input("URL 1:", key="yt1", placeholder="https://www.youtube.com/watch?v=...")
    youtube_url_2 = st.text_input("URL 2:", key="yt2")
    youtube_url_3 = st.text_input("URL 3:", key="yt3")
    if any([youtube_url_1, youtube_url_2, youtube_url_3]): st.session_state.last_input_type = 'youtube'
manual_text = st.text_area("Ou cole os coment√°rios (um por linha):", key="manual", height=150)
if manual_text: st.session_state.last_input_type = 'manual'

all_comments_list = []
if st.session_state.last_input_type == 'file' and uploaded_file:
    all_comments_list = extract_text_from_file(uploaded_file.read(), os.path.splitext(uploaded_file.name)[1].lower())
elif st.session_state.last_input_type == 'youtube':
    youtube_urls = [url.strip() for url in [youtube_url_1, youtube_url_2, youtube_url_3] if url.strip()]
    if youtube_urls:
        temp_comments = []
        for url in youtube_urls: temp_comments.extend(download_youtube_comments(url))
        all_comments_list = temp_comments
elif st.session_state.last_input_type == 'manual' and manual_text:
    all_comments_list = [line for line in manual_text.split("\n") if line.strip()]

if all_comments_list:
    original_comment_count = len(all_comments_list)
    comments_for_analysis = all_comments_list[:MAX_COMMENTS_TO_PROCESS]
    st.success(f"{original_comment_count} coment√°rios carregados. Analisando uma amostra de {len(comments_for_analysis)}.")
    text_to_analyze = "\n".join(comments_for_analysis)
    analysis_results = analyze_text_with_gemini(text_to_analyze)

    if analysis_results:
        tabs = st.tabs(["üìä Sentimento", "üí° Temas", "üîë Termos-Chave", "üîó Rela√ß√µes", "üìù An√°lise Qualitativa", "üßë‚Äçüíº Persona", "üöÄ Growth", "üìà PMM"])

        with tabs[0]:
            st.subheader("Gr√°fico de Sentimento Geral")
            plot_sentiment_chart(analysis_results.get('sentiment', {}))
            st.subheader("An√°lise dos Insights do Gr√°fico")
            with st.spinner("Gerando an√°lise..."):
                st.markdown(generate_sentiment_analysis_text(analysis_results.get('sentiment', {})))
        with tabs[1]:
            st.subheader("Gr√°fico de Temas por Sentimento")
            plot_topics_chart(analysis_results.get('topics', []))
            st.subheader("An√°lise Estrat√©gica dos Temas")
            with st.spinner("Gerando an√°lise..."):
                st.markdown(generate_topics_analysis_text(analysis_results.get('topics', [])))
        with tabs[2]:
            st.subheader("Nuvem de Termos-Chave")
            plot_word_cloud(analysis_results.get('term_clusters', {}))
            st.subheader("An√°lise dos Insights do Gr√°fico")
            with st.spinner("Gerando an√°lise..."):
                st.markdown(generate_wordcloud_analysis_text(analysis_results.get('term_clusters', {})))
        with tabs[3]:
            st.subheader("Grafo de Rela√ß√£o Entre Temas")
            plot_topic_relations_chart(analysis_results.get('topic_relations', []))
            st.subheader("An√°lise dos Insights do Gr√°fico")
            with st.spinner("Gerando an√°lise..."):
                st.markdown(generate_relations_analysis_text(analysis_results.get('topic_relations', [])))
        with tabs[4]:
            st.subheader("An√°lise Qualitativa Geral")
            with st.spinner("Gerando an√°lise..."):
                st.markdown(generate_qualitative_analysis(analysis_results, text_to_analyze))
        with tabs[5]:
            st.subheader("Perfil da Persona Sint√©tica")
            with st.spinner("Gerando persona..."):
                st.markdown(generate_persona_insights(analysis_results, text_to_analyze))
        with tabs[6]:
            st.subheader("Sugest√µes de Testes de Growth (ICE Score)")
            with st.spinner("Gerando testes..."):
                ice = generate_ice_score_tests(analysis_results)
                if ice: st.dataframe(pd.DataFrame(ice), hide_index=True, use_container_width=True)
                else: st.warning("N√£o foi poss√≠vel gerar os testes de Growth.")
        with tabs[7]:
            st.subheader("Briefing Estrat√©gico para Produto e Marketing")
            with st.spinner("Gerando briefing..."):
                st.markdown(generate_product_marketing_insights(analysis_results))
    else:
        st.error("A an√°lise com a IA falhou. Verifique os dados ou tente novamente.")
else:
    st.info("Aguardando dados para iniciar a an√°lise...")

# --- FOOTER ---
st.markdown("---")
st.subheader("üí° Gostou da aplica√ß√£o?")
st.markdown("Para acessar a vers√£o completa e sem limita√ß√µes, clique no bot√£o abaixo.")
st.link_button("Acessar The Research AI", "https://www.theresearchai.online/", type="primary")
st.markdown("---")
st.markdown("Desenvolvido por Pedro Costa")
