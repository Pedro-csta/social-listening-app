import streamlit as st

import pandas as pd

import json

import io

from docx import Document

from youtube_comment_downloader import YoutubeCommentDownloader

import re

import matplotlib.pyplot as plt

import seaborn as sns

import networkx as nx

from wordcloud import WordCloud

import os

import google.generativeai as genai



# --- CONFIGURAÃ‡ÃƒO GEMINI ---

gemini_api_key = st.secrets.get("GOOGLE_API_KEY")

if not gemini_api_key:

Â  Â  st.error("A chave da API do Google Gemini nÃ£o foi encontrada. Configure-a no 'secrets.toml' ou como variÃ¡vel de ambiente GOOGLE_API_KEY.")

Â  Â  st.stop()

genai.configure(api_key=gemini_api_key)

model = genai.GenerativeModel('gemini-2.0-flash')



# --- CONSTANTE PARA LIMITE DE COMENTÃRIOS ---

MAX_COMMENTS_TO_PROCESS = 1000 # Limite de 1000 comentÃ¡rios



# --- EXTRAÃ‡ÃƒO DE DADOS ---

@st.cache_data(show_spinner=False)

def extract_text_from_file(file_contents, file_extension):

Â  Â  text_content_list = []

Â  Â  try:

Â  Â  Â  Â  if file_extension == '.csv':

Â  Â  Â  Â  Â  Â  df = pd.read_csv(io.StringIO(file_contents.decode('utf-8')))

Â  Â  Â  Â  Â  Â  if 'comentario' in df.columns:

Â  Â  Â  Â  Â  Â  Â  Â  text_content_list = df['comentario'].dropna().astype(str).tolist()

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  for _, row in df.iterrows():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text_content_list.append(" ".join(row.dropna().astype(str).tolist()))

Â  Â  Â  Â  elif file_extension in ['.xls', '.xlsx']:

Â  Â  Â  Â  Â  Â  df = pd.read_excel(io.BytesIO(file_contents))

Â  Â  Â  Â  Â  Â  if 'comentario' in df.columns:

Â  Â  Â  Â  Â  Â  Â  Â  text_content_list = df['comentario'].dropna().astype(str).tolist()

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  for _, row in df.iterrows():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text_content_list.append(" ".join(row.dropna().astype(str).tolist()))

Â  Â  Â  Â  elif file_extension in ['.doc', '.docx']:

Â  Â  Â  Â  Â  Â  document = Document(io.BytesIO(file_contents))

Â  Â  Â  Â  Â  Â  for para in document.paragraphs:

Â  Â  Â  Â  Â  Â  Â  Â  if para.text.strip():

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  text_content_list.append(para.text)

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  st.warning(f"Formato de arquivo nÃ£o suportado: {file_extension}.")

Â  Â  Â  Â  Â  Â  return []

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro ao extrair texto do arquivo: {e}")

Â  Â  Â  Â  return []

Â  Â Â 

Â  Â  return text_content_list



@st.cache_data(show_spinner=False)

def download_youtube_comments(youtube_url):

Â  Â  try:

Â  Â  Â  Â  downloader = YoutubeCommentDownloader()

Â  Â  Â  Â  video_id = None

Â  Â  Â  Â  match = re.search(r'(?:v=|/)([0-9A-Za-z_-]{11}).*', youtube_url)

Â  Â  Â  Â  if match:

Â  Â  Â  Â  Â  Â  video_id = match.group(1)

Â  Â  Â  Â  if not video_id:

Â  Â  Â  Â  Â  Â  st.error(f"URL do YouTube invÃ¡lida: {youtube_url}.")

Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â Â 

Â  Â  Â  Â  with st.spinner(f"Baixando comentÃ¡rios do vÃ­deo: {youtube_url}..."):

Â  Â  Â  Â  Â  Â  all_comments = []

Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  comments_generator = downloader.get_comments(video_id)

Â  Â  Â  Â  Â  Â  Â  Â  for comment in comments_generator:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_comments.append(comment['text'])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Adicione um limite interno para o downloader do YouTube para evitar downloads excessivos

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(all_comments) >= MAX_COMMENTS_TO_PROCESS * 2: # Baixa um pouco mais para ter certeza de atingir o limite

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"NÃ£o foi possÃ­vel baixar comentÃ¡rios: {e}")

Â  Â  Â  Â  Â  Â  Â  Â  return []

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return all_comments



Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro geral ao baixar comentÃ¡rios: {e}.")

Â  Â  Â  Â  return []



# --- ANÃLISE COM GEMINI ---

@st.cache_data(show_spinner=True)

def analyze_text_with_gemini(text_to_analyze):

Â  Â  if not text_to_analyze.strip():

Â  Â  Â  Â  return {

Â  Â  Â  Â  Â  Â  "sentiment": {"positive": 0.0, "neutral": 0.0, "negative": 0.0, "no_sentiment_detected": 100.0},

Â  Â  Â  Â  Â  Â  "topics": [],

Â  Â  Â  Â  Â  Â  "term_clusters": {},

Â  Â  Â  Â  Â  Â  "topic_relations": []

Â  Â  Â  Â  }

Â  Â Â 

Â  Â  num_comments_in_prompt = len(text_to_analyze.split('\n'))

Â  Â Â 

Â  Â  prompt = f"""

Analise o texto de comentÃ¡rios de redes sociais abaixo de forma estritamente objetiva, factual e consistente.

Estes {num_comments_in_prompt} comentÃ¡rios sÃ£o uma amostra ou o total de comentÃ¡rios disponÃ­veis para anÃ¡lise.

Extraia as informaÃ§Ãµes solicitadas. Calcule as porcentagens e contagens EXATAS com base no total de comentÃ¡rios relevantes.

ForneÃ§a as seguintes informaÃ§Ãµes em formato JSON, exatamente como a estrutura definida. NÃ£o inclua nenhum texto adicional antes ou depois do JSON.

1. Sentimento Geral: A porcentagem de comentÃ¡rios classificados como 'Positivo', 'Neutro', 'Negativo' e 'Sem Sentimento Detectado'. As porcentagens devem somar 100%.

2. Temas Mais Citados: Lista de 5 a 10 temas principais discutidos nos comentÃ¡rios, com contagem EXATA de comentÃ¡rios Positivos, Neutros e Negativos.

3. Agrupamento de Termos/Nuvem de Palavras: Lista dos 10 a 20 termos ou palavras-chave mais frequentes. Para cada termo, forneÃ§a a frequÃªncia (contagem de ocorrÃªncias).

4. RelaÃ§Ã£o entre Temas: Liste 3 a 5 pares de temas que frequentemente aparecem juntos, indicando relaÃ§Ã£o clara e lÃ³gica.

O JSON deve ter a seguinte estrutura:

{{

Â  "sentiment": {{

Â  Â  "positive": float,

Â  Â  "neutral": float,

Â  Â  "negative": float,

Â  Â  "no_sentiment_detected": float

Â  }},

Â  "topics": [

Â  Â  {{

Â  Â  Â  "name": "Nome do Tema",

Â  Â  Â  "positive": int,

Â  Â  Â  "neutral": int,

Â  Â  Â  "negative": int

Â  Â  }}

Â  ],

Â  "term_clusters": {{

Â  Â  "termo1": int,

Â  Â  "termo2": int

Â  }},

Â  "topic_relations": [

Â  Â  {{

Â  Â  Â  "source": "Tema A",

Â  Â  Â  "target": "Tema B",

Â  Â  Â  "description": "Breve descriÃ§Ã£o da relaÃ§Ã£o"

Â  Â  }}

Â  ]

}}

Texto para anÃ¡lise:

"{text_to_analyze}"

"""

Â  Â  try:

Â  Â  Â  Â  response = model.generate_content(prompt)

Â  Â  Â  Â  response_text = response.text.strip()

Â  Â  Â  Â  if response_text.startswith("```json"):

Â  Â  Â  Â  Â  Â  response_text = response_text[len("```json"):].strip()

Â  Â  Â  Â  if response_text.endswith("```"):

Â  Â  Â  Â  Â  Â  response_text = response_text[:-len("```")].strip()

Â  Â  Â  Â  data = json.loads(response_text)

Â  Â  Â  Â Â 

Â  Â  Â  Â  if 'no_sentiment_detected' not in data['sentiment']:

Â  Â  Â  Â  Â  Â  total = data['sentiment'].get('positive', 0) + data['sentiment'].get('neutral', 0) + data['sentiment'].get('negative', 0)

Â  Â  Â  Â  Â  Â  data['sentiment']['no_sentiment_detected'] = round(100.0 - total, 2)

Â  Â  Â  Â Â 

Â  Â  Â  Â  total_sum = sum(data['sentiment'].values())

Â  Â  Â  Â  if total_sum != 100 and total_sum != 0:

Â  Â  Â  Â  Â  Â  for key in data['sentiment']:

Â  Â  Â  Â  Â  Â  Â  Â  data['sentiment'][key] = round(data['sentiment'][key] / total_sum * 100, 2)

Â  Â  Â  Â  return data

Â  Â  except json.JSONDecodeError as e:

Â  Â  Â  Â  st.error(f"Erro ao decodificar JSON da resposta do Gemini: {e}")

Â  Â  Â  Â  st.code(f"Resposta bruta do Gemini (AnÃ¡lise): {response_text}")

Â  Â  Â  Â  return None

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro inesperado ao analisar com Gemini: {e}")

Â  Â  Â  Â  return None



@st.cache_data(show_spinner=True)

def generate_qualitative_analysis(analysis_results, original_text_sample):

Â  Â  sentiment = analysis_results.get('sentiment', {})

Â  Â  topics = analysis_results.get('topics', [])

Â  Â  term_clusters = analysis_results.get('term_clusters', {})

Â  Â  topic_relations = analysis_results.get('topic_relations', [])

Â  Â  prompt = f"""

Com base na anÃ¡lise de social listening dos comentÃ¡rios, atue como um especialista em Marketing de Produto, Social Listening e Data Analysis. Redija uma anÃ¡lise qualitativa abrangente em atÃ© 4 parÃ¡grafos, focando nos aprendizados e insights estratÃ©gicos.

Considere as seguintes informaÃ§Ãµes:

Sentimento Geral: {json.dumps(sentiment, ensure_ascii=False)}

Temas Mais Citados: {json.dumps(topics, ensure_ascii=False)}

Agrupamento de Termos: {json.dumps(term_clusters, ensure_ascii=False)}

RelaÃ§Ã£o entre Temas: {json.dumps(topic_relations, ensure_ascii=False)}

"""

Â  Â  try:

Â  Â  Â  Â  response = model.generate_content(prompt)

Â  Â  Â  Â  return response.text.strip()

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro na anÃ¡lise qualitativa: {e}")

Â  Â  Â  Â  return "NÃ£o foi possÃ­vel gerar a anÃ¡lise qualitativa."



@st.cache_data(show_spinner=True)

def generate_persona_insights(analysis_results, original_text_sample):

Â  Â  sentiment = analysis_results.get('sentiment', {})

Â  Â  topics = analysis_results.get('topics', [])

Â  Â  term_clusters = analysis_results.get('term_clusters', {})

Â  Â  original_text_display = original_text_sample[:1000] + "..." if len(original_text_sample) > 1000 else original_text_sample

Â  Â  combined_context = f"""

ComentÃ¡rios originais (amostra): {original_text_display}

Resultados da anÃ¡lise:

- Sentimento Geral: {json.dumps(sentiment, ensure_ascii=False)}

- Temas Mais Citados: {json.dumps(topics, ensure_ascii=False)}

- Agrupamento de Termos: {json.dumps(term_clusters, ensure_ascii=False)}

"""

Â  Â  persona_prompt = f"""

Considerando a anÃ¡lise de social listening de um pÃºblico e o conceito de "personas sintÃ©ticas" (personas criadas a partir de dados comportamentais e sentimentos reais), crie um insight de persona sintÃ©tica para o pÃºblico desses comentÃ¡rios.

Descreva:

- As principais dores e necessidades desse pÃºblico;

- Seus interesses ou paixÃµes (explÃ­citas ou implÃ­citas);

- O tom predominante da comunicaÃ§Ã£o (positivo, negativo, neutro, impaciente, engajado, etc);

- Oportunidades para engajar ou atender melhor essa persona;

- DÃª um nome sugestivo Ã  persona, como "O CrÃ­tico Construtivo" ou "A Consumidora Engajada".

Use atÃ© 3-4 parÃ¡grafos.Â Â 

Contexto da anÃ¡lise:

{combined_context}

"""

Â  Â  try:

Â  Â  Â  Â  response = model.generate_content(persona_prompt)

Â  Â  Â  Â  return response.text.strip()

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro ao gerar insights para persona sintÃ©tica: {e}")

Â  Â  Â  Â  return "NÃ£o foi possÃ­vel gerar insights de persona sintÃ©tica."



@st.cache_data(show_spinner=True)

def generate_ice_score_tests(analysis_results):

Â  Â  sentiment = analysis_results.get('sentiment', {})

Â  Â  topics = analysis_results.get('topics', [])

Â  Â  term_clusters = analysis_results.get('term_clusters', {})

Â  Â  topic_relations = analysis_results.get('topic_relations', [])

Â  Â  prompt = f"""

Com base na anÃ¡lise de social listening fornecida, atue como um Growth Hacker experiente. Sugira EXATAMENTE 10 testes de Growth priorizados usando a metodologia ICE Score (Impacto, ConfianÃ§a, Facilidade).

Para cada teste, identifique UMA ÃšNICA VARIÃVEL de alavancagem principal entre: "Canal", "SegmentaÃ§Ã£o", "Formato", "Criativo" ou "Copy/Argumento".

Apresente os resultados em formato JSON, como uma lista de objetos. NÃ£o inclua nenhum texto adicional antes ou depois do JSON.

A lista de testes deve ser ordenada do maior para o menor ICE Score.

InformaÃ§Ãµes da anÃ¡lise:

Sentimento Geral: {json.dumps(sentiment, ensure_ascii=False)}

Temas Mais Citados: {json.dumps(topics, ensure_ascii=False)}

Agrupamento de Termos: {json.dumps(term_clusters, ensure_ascii=False)}

RelaÃ§Ã£o entre Temas: {json.dumps(topic_relations, ensure_ascii=False)}

Exemplo:

[

Â  {{

Â  Â  "Ordem": 1,

Â  Â  "Nome do Teste": "Teste de Manchete de AnÃºncio",

Â  Â  "DescriÃ§Ã£o do Teste": "Testar diferentes manchetes para anÃºncios no Facebook Ads para aumentar o CTR, focando no tema 'BenefÃ­cios PrevidenciÃ¡rios'.",

Â  Â  "VariÃ¡vel de Alavancagem": "Copy/Argumento",

Â  Â  "Impacto (1-10)": 9,

Â  Â  "ConfianÃ§a (1-10)": 8,

Â  Â  "Facilidade (1-10)": 7,

Â  Â  "ICE Score": 8.00

Â  }}

]

"""

Â  Â  try:

Â  Â  Â  Â  response = model.generate_content(prompt)

Â  Â  Â  Â  response_text = response.text.strip()

Â  Â  Â  Â  if response_text.startswith("```json"):

Â  Â  Â  Â  Â  Â  response_text = response_text[len("```json"):].strip()

Â  Â  Â  Â  if response_text.endswith("```"):

Â  Â  Â  Â  Â  Â  response_text = response_text[:-len("```")].strip()

Â  Â  Â  Â  data = json.loads(response_text)

Â  Â  Â  Â  return data

Â  Â  except json.JSONDecodeError as e:

Â  Â  Â  Â  st.error(f"Erro ao gerar ICE Score: {e}")

Â  Â  Â  Â  st.code(f"Resposta bruta do Gemini (ICE Score): {response_text}")

Â  Â  Â  Â  return None

Â  Â  except Exception as e:

Â  Â  Â  Â  st.error(f"Erro inesperado ao gerar testes de Growth com ICE Score: {e}")

Â  Â  Â  Â  return None



# --- VISUALIZAÃ‡ÃƒO ---

def plot_sentiment_chart(sentiment_data):

Â  Â  labels_order = ['positive', 'neutral', 'negative', 'no_sentiment_detected']

Â  Â  display_labels = ['Positivo', 'Neutro', 'Negativo', 'NÃ£o Detectado']

Â  Â  colors_for_pie = {

Â  Â  Â  Â  'positive': '#ff99b0',

Â  Â  Â  Â  'neutral': '#1f2329',

Â  Â  Â  Â  'negative': '#fe1874',

Â  Â  Â  Â  'no_sentiment_detected': '#cccccc'

Â  Â  }

Â  Â  sizes = [sentiment_data.get(label, 0.0) for label in labels_order]

Â  Â  filtered_data = [(display_labels[i], sizes[i], colors_for_pie[labels_order[i]])

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â for i, size in enumerate(sizes) if size > 0]

Â  Â  if not filtered_data:

Â  Â  Â  Â  st.warning("Dados insuficientes para grÃ¡fico de sentimento.")

Â  Â  Â  Â  return

Â  Â  filtered_labels, filtered_sizes, filtered_colors = zip(*filtered_data)

Â  Â  explode = [0.03] * len(filtered_labels)

Â  Â  fig, ax = plt.subplots(figsize=(6, 6))

Â  Â  wedges, texts, autotexts = ax.pie(

Â  Â  Â  Â  filtered_sizes,

Â  Â  Â  Â  explode=explode,

Â  Â  Â  Â  labels=filtered_labels,

Â  Â  Â  Â  colors=filtered_colors,

Â  Â  Â  Â  autopct='%1.1f%%',

Â  Â  Â  Â  startangle=90,

Â  Â  Â  Â  pctdistance=0.85

Â  Â  )

Â  Â  for autotext in autotexts:

Â  Â  Â  Â  autotext.set_color('#f3f3f3')

Â  Â  Â  Â  autotext.set_fontsize(12)

Â  Â  Â  Â  autotext.set_fontweight('bold')

Â  Â  for text in texts:

Â  Â  Â  Â  text.set_color('#1f2329')

Â  Â  Â  Â  text.set_fontsize(10)

Â  Â  centre_circle = plt.Circle((0,0),0.70,fc='#f3f3f3')

Â  Â  fig.gca().add_artist(centre_circle)

Â  Â  ax.axis('equal')

Â  Â  ax.set_title('1. AnÃ¡lise de Sentimento Geral', pad=18, color='#1f2329')

Â  Â  st.pyplot(fig)



def plot_topics_chart(topics_data):

Â  Â  if not topics_data:

Â  Â  Â  Â  st.warning("Dados de temas insuficientes.")

Â  Â  Â  Â  return

Â  Â  df_topics = pd.DataFrame(topics_data)

Â  Â  df_topics['positive'] = df_topics['positive'].fillna(0).astype(int)

Â  Â  df_topics['neutral'] = df_topics['neutral'].fillna(0).astype(int)

Â  Â  df_topics['negative'] = df_topics['negative'].fillna(0).astype(int)

Â  Â  df_topics['Total'] = df_topics['positive'] + df_topics['neutral'] + df_topics['negative']

Â  Â  df_topics = df_topics.sort_values('Total', ascending=True)

Â  Â  fig, ax = plt.subplots(figsize=(8, max(4, len(df_topics) * 0.5)))

Â  Â  bar_colors = ['#ff99b0', '#1f2329', '#fe1874']

Â  Â  df_topics[['positive', 'neutral', 'negative']].plot(

Â  Â  Â  Â  kind='barh',

Â  Â  Â  Â  stacked=True,

Â  Â  Â  Â  color=bar_colors,

Â  Â  Â  Â  ax=ax

Â  Â  )

Â  Â  ax.set_title('2. Temas Mais Citados por Sentimento', color='#1f2329')

Â  Â  ax.set_xlabel('NÃºmero de ComentÃ¡rios', color='#1f2329')

Â  Â  ax.set_ylabel('Tema', color='#1f2329')

Â  Â  ax.set_yticklabels(df_topics['name'], color='#1f2329')

Â  Â  ax.tick_params(axis='x', colors='#1f2329')

Â  Â  ax.tick_params(axis='y', colors='#1f2329')

Â  Â  ax.legend(['Positivo', 'Neutro', 'Negativo'], loc='lower right', frameon=False, labelcolor='#1f2329')

Â  Â  plt.tight_layout()

Â  Â  st.pyplot(fig)



def plot_word_cloud(term_clusters_data):

Â  Â  if not term_clusters_data:

Â  Â  Â  Â  st.warning("Dados de termos insuficientes para nuvem de palavras.")

Â  Â  Â  Â  return

Â  Â  def color_func(word, font_size, position, orientation, random_state=None, **kwargs):

Â  Â  Â  Â  import random

Â  Â  Â  Â  return '#fe1874' if random_state and random_state.randint(0, 2) == 0 else '#1f2329'

Â  Â  wordcloud = WordCloud(

Â  Â  Â  Â  width=700,

Â  Â  Â  Â  height=400,

Â  Â  Â  Â  background_color='#f3f3f3',

Â  Â  Â  Â  color_func=color_func,

Â  Â  Â  Â  min_font_size=12,

Â  Â  Â  Â  max_words=60,

Â  Â  Â  Â  prefer_horizontal=0.8,

Â  Â  Â  Â  collocations=False

Â  Â  ).generate_from_frequencies(term_clusters_data)

Â  Â  fig = plt.figure(figsize=(8, 5))

Â  Â  plt.imshow(wordcloud, interpolation='bilinear')

Â  Â  plt.axis('off')

Â  Â  plt.title('3. Agrupamento de Termos (Nuvem de Palavras)', pad=16, fontsize=15)

Â  Â  st.pyplot(fig)



def plot_topic_relations_chart(topic_relations_data):

Â  Â  if not topic_relations_data:

Â  Â  Â  Â  st.warning("Dados insuficientes para grafo de relaÃ§Ã£o entre temas.")

Â  Â  Â  Â  return

Â  Â  G = nx.Graph()

Â  Â  for rel in topic_relations_data:

Â  Â  Â  Â  source = rel.get('source')

Â  Â  Â  Â  target = rel.get('target')

Â  Â  Â  Â  description = rel.get('description')

Â  Â  Â  Â  if source and target:

Â  Â  Â  Â  Â  Â  G.add_edge(source, target, description=description)

Â  Â  if not G.edges():

Â  Â  Â  Â  st.warning("Nenhuma relaÃ§Ã£o vÃ¡lida encontrada para construir o grafo de rede.")

Â  Â  Â  Â  return

Â  Â  fig, ax = plt.subplots(figsize=(8, 7))

Â  Â  pos = nx.spring_layout(G, k=0.7, iterations=50, seed=42)

Â  Â  node_colors = ['#fe1874' for _ in G.nodes()]

Â  Â  nx.draw_networkx_nodes(G, pos, node_size=2000, node_color=node_colors, alpha=0.9, ax=ax)

Â  Â  nx.draw_networkx_edges(G, pos, width=1.2, edge_color='#1f2329', alpha=0.6, ax=ax)

Â  Â  nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold', font_color='#1f2329', ax=ax)

Â  Â  ax.set_title('4. RelaÃ§Ã£o Entre Temas (Grafo de Rede)', pad=16, color='#1f2329')

Â  Â  plt.axis('off')

Â  Â  plt.tight_layout()

Â  Â  st.pyplot(fig)



# --- APP STREAMLIT ---

st.set_page_config(layout="wide", page_title="Social Listening Tool + AI")

st.title("ğŸ—£ï¸ Social Listening Tool + AI")

st.markdown("---")



st.markdown(f"Carregue uma base de comentÃ¡rios (.csv, .xls, .xlsx, .doc, .docx), uma URL de vÃ­deo do YouTube, ou cole comentÃ¡rios no campo abaixo. **SerÃ£o processados no mÃ¡ximo {MAX_COMMENTS_TO_PROCESS} comentÃ¡rios para anÃ¡lise pela IA.**")



all_comments_list = []

original_comment_count = 0 # VariÃ¡vel para armazenar a contagem original antes do corte



col1, col2 = st.columns(2)

with col1:

Â  Â  uploaded_file = st.file_uploader(

Â  Â  Â  Â  "FaÃ§a upload do arquivo de comentÃ¡rios (.csv, .xls, .xlsx, .doc, .docx):",

Â  Â  Â  Â  type=["csv", "xls", "xlsx", "doc", "docx"],

Â  Â  Â  Â  key="fileuploader"

Â  Â  )

Â  Â  if uploaded_file is not None:

Â  Â  Â  Â  file_extension = os.path.splitext(uploaded_file.name)[1].lower()

Â  Â  Â  Â  file_contents = uploaded_file.read()

Â  Â  Â  Â  extracted_comments = extract_text_from_file(file_contents, file_extension)

Â  Â  Â  Â  original_comment_count = len(extracted_comments) # Armazena a contagem antes do corte

Â  Â  Â  Â  all_comments_list = extracted_comments[:MAX_COMMENTS_TO_PROCESS]



with col2:

Â  Â  youtube_url_input = st.text_input("Ou insira uma URL de vÃ­deo do YouTube:")

Â  Â  if youtube_url_input:

Â  Â  Â  Â  yt_comments = download_youtube_comments(youtube_url_input.strip())

Â  Â  Â  Â  if yt_comments:

Â  Â  Â  Â  Â  Â  original_comment_count = len(yt_comments) # Armazena a contagem antes do corte

Â  Â  Â  Â  Â  Â  all_comments_list = yt_comments[:MAX_COMMENTS_TO_PROCESS]



manual_text = st.text_area("Ou cole comentÃ¡rios (um por linha):")

if manual_text and not all_comments_list:

Â  Â  manual_comments = [l for l in manual_text.split("\n") if l.strip()]

Â  Â  if manual_comments:

Â  Â  Â  Â  original_comment_count = len(manual_comments) # Armazena a contagem antes do corte

Â  Â  Â  Â  all_comments_list = manual_comments[:MAX_COMMENTS_TO_PROCESS]

Â  Â  Â  Â  st.success(f"{len(all_comments_list)} comentÃ¡rios colados e prontos para anÃ¡lise.")



if all_comments_list:

Â  Â  # Mensagem na tarja verde ajustada

Â  Â  if original_comment_count > MAX_COMMENTS_TO_PROCESS:

Â  Â  Â  Â  st.success(f"ComentÃ¡rios carregados! O limite de {MAX_COMMENTS_TO_PROCESS} comentÃ¡rios processados nessa versÃ£o teste da ferramenta foi atingido. Esse limite Ã© importante para garantir que todos os usuÃ¡rios tenham acesso a um ambiente de teste estÃ¡vel.")

Â  Â  Â  Â  st.info(f"SerÃ£o processados os primeiros {len(all_comments_list)} comentÃ¡rios para anÃ¡lise.")

Â  Â  else:

Â  Â  Â  Â  st.success(f"ComentÃ¡rios carregados! SerÃ£o processados {len(all_comments_list)} comentÃ¡rios para anÃ¡lise.")



Â  Â  text_to_analyze = "\n".join(all_comments_list)

Â  Â  with st.spinner("Processando anÃ¡lise com Gemini..."):

Â  Â  Â  Â  analysis_results = analyze_text_with_gemini(text_to_analyze)

Â  Â  if analysis_results:

Â  Â  Â  Â  tabs = st.tabs([

Â  Â  Â  Â  Â  Â  "ğŸ“Š Sentimento",

Â  Â  Â  Â  Â  Â  "ğŸ’¡ Temas",

Â  Â  Â  Â  Â  Â  "ğŸ”‘ Termos-Chave",

Â  Â  Â  Â  Â  Â  "ğŸ”— RelaÃ§Ãµes entre Temas",

Â  Â  Â  Â  Â  Â  "ğŸ“ AnÃ¡lise Qualitativa",

Â  Â  Â  Â  Â  Â  "ğŸ§‘â€ğŸ’¼ Persona SintÃ©tica",

Â  Â  Â  Â  Â  Â  "ğŸš€ Testes de Growth (ICE Score)"

Â  Â  Â  Â  ])

Â  Â  Â  Â  with tabs[0]:

Â  Â  Â  Â  Â  Â  st.subheader("Sentimento Geral")

Â  Â  Â  Â  Â  Â  plot_sentiment_chart(analysis_results.get('sentiment', {}))

Â  Â  Â  Â  Â  Â  with st.expander("Ver dados brutos de sentimento"):

Â  Â  Â  Â  Â  Â  Â  Â  st.json(analysis_results.get('sentiment', {}))

Â  Â  Â  Â  with tabs[1]:

Â  Â  Â  Â  Â  Â  st.subheader("Temas mais Citados com Sentimento")

Â  Â  Â  Â  Â  Â  plot_topics_chart(analysis_results.get('topics', []))

Â  Â  Â  Â  Â  Â  with st.expander("Ver dados brutos dos temas"):

Â  Â  Â  Â  Â  Â  Â  Â  st.json(analysis_results.get('topics', []))

Â  Â  Â  Â  with tabs[2]:

Â  Â  Â  Â  Â  Â  st.subheader("Agrupamento de Termos/Nuvem de Palavras")

Â  Â  Â  Â  Â  Â  plot_word_cloud(analysis_results.get('term_clusters', {}))

Â  Â  Â  Â  Â  Â  with st.expander("Ver dados brutos de termos-chave"):

Â  Â  Â  Â  Â  Â  Â  Â  st.json(analysis_results.get('term_clusters', {}))

Â  Â  Â  Â  with tabs[3]:

Â  Â  Â  Â  Â  Â  st.subheader("RelaÃ§Ã£o entre Temas (Grafo de Rede)")

Â  Â  Â  Â  Â  Â  plot_topic_relations_chart(analysis_results.get('topic_relations', []))

Â  Â  Â  Â  Â  Â  with st.expander("Ver dados brutos das relaÃ§Ãµes entre temas"):

Â  Â  Â  Â  Â  Â  Â  Â  st.json(analysis_results.get('topic_relations', []))

Â  Â  Â  Â  with tabs[4]:

Â  Â  Â  Â  Â  Â  st.subheader("AnÃ¡lise Qualitativa")

Â  Â  Â  Â  Â  Â  with st.spinner("Gerando anÃ¡lise qualitativa..."):

Â  Â  Â  Â  Â  Â  Â  Â  qualitative = generate_qualitative_analysis(analysis_results, text_to_analyze)

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(qualitative)

Â  Â  Â  Â  with tabs[5]:

Â  Â  Â  Â  Â  Â  st.subheader("Persona SintÃ©tica (Insights)")

Â  Â  Â  Â  Â  Â  with st.spinner("Gerando insights de persona..."):

Â  Â  Â  Â  Â  Â  Â  Â  persona = generate_persona_insights(analysis_results, text_to_analyze)

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(persona)

Â  Â  Â  Â  with tabs[6]:

Â  Â  Â  Â  Â  Â  st.subheader("SugestÃµes de Testes de Growth (ICE Score)")

Â  Â  Â  Â  Â  Â  with st.spinner("Gerando sugestÃµes de testes de growth..."):

Â  Â  Â  Â  Â  Â  Â  Â  ice = generate_ice_score_tests(analysis_results)

Â  Â  Â  Â  Â  Â  Â  Â  if ice:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_ice = pd.DataFrame(ice)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cols = ["Ordem", "Nome do Teste", "DescriÃ§Ã£o do Teste", "VariÃ¡vel de Alavancagem", "Impacto (1-10)", "ConfianÃ§a (1-10)", "Facilidade (1-10)", "ICE Score"]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_ice = df_ice[[c for c in cols if c in df_ice.columns]]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_ice = df_ice.sort_values(by="ICE Score", ascending=False)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(df_ice, hide_index=True, use_container_width=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("Ver dados brutos dos testes de growth (ICE Score)"):

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.json(ice)

Â  Â  Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("NÃ£o foi possÃ­vel gerar sugestÃµes de testes de growth.")

Â  Â  else:

Â  Â  Â  Â  st.error("NÃ£o foi possÃ­vel gerar a anÃ¡lise com Gemini. Reveja os dados e tente novamente.")

else:

Â  Â  st.info("FaÃ§a o upload de comentÃ¡rios, cole manualmente ou insira uma URL do YouTube para iniciar a anÃ¡lise.")



# --- FOOTER / SEÃ‡ÃƒO DE CAPTAÃ‡ÃƒO DE E-MAIL COM TALLY ---

st.markdown("---") # Linha divisÃ³ria para separar do conteÃºdo principal



st.subheader("ğŸ’¡ Gostou de testar a aplicaÃ§Ã£o?")

st.markdown("""

Â  Â  Essa versÃ£o de teste possuÃ­ uma limitaÃ§Ã£o de comentÃ¡rios que podem ser analisados e de volume de anÃ¡lises por dia.



Â  Â  Caso tenha interesse em acessar a aplicaÃ§Ã£o completa, clique aqui para conhecer a versÃ£o final da aplicaÃ§Ã£o.

""")



# Link para a ferramenta

TALLY_FORM_URL = "https://www.theresearchai.online/"



# Estilo para simular o botÃ£o "Browse files"

# As cores e paddings sÃ£o aproximadas do tema claro do Streamlit.

st.markdown(f"""

<a href="{TALLY_FORM_URL}" target="_blank" style="

Â  Â  display: inline-flex;

Â  Â  align-items: center;

Â  Â  justify-content: center;

Â  Â  padding: 0.25rem 0.75rem; /* Ajustado para parecer mais com o padding do browse files */

Â  Â  border-radius: 0.25rem; /* Ajustado para um arredondamento padrÃ£o do Streamlit */

Â  Â  border: 1px solid rgba(0, 0, 0, 0.2); /* Borda cinza clara */

Â  Â  color: rgb(58, 93, 255); /* Azul/roxo do Streamlit, pode variar com o tema */

Â  Â  background-color: rgb(240, 242, 246); /* Fundo cinza claro */

Â  Â  font-weight: 400;

Â  Â  font-size: 1rem;

Â  Â  line-height: 1.6;

Â  Â  text-decoration: none;

Â  Â  cursor: pointer;

Â  Â  transition: background-color 0.1s ease 0s, border-color 0.1s ease 0s;

">

Â  Clique aqui e acesse a ferramenta!

</a>

""", unsafe_allow_html=True)



st.markdown("---") # Outra linha divisÃ³ria no final



st.markdown("Desenvolvido com Python, â¤ï¸ e AI por Pedro Costa | Product Marketing & Martech Specialist")
